wandb: Currently logged in as: joschka (nextaids). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.5
wandb: Run data is saved locally in /home/jgross/kinodata-docked-rescore/wandb/run-20230219_145817-8ge9d36q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scarlet-sun-42
wandb: â­ï¸ View project at https://wandb.ai/nextaids/kinodata-docked-rescore
wandb: ðŸš€ View run at https://wandb.ai/nextaids/kinodata-docked-rescore/runs/8ge9d36q
/main/home/mambaforge/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:285: UserWarning: Providing log_model=all requires wandb version >= 0.10.22 for logging associated model metadata.
Hint: Upgrade with `pip install --upgrade wandb`.
  rank_zero_warn(
/main/home/mambaforge/lib/python3.9/site-packages/torch_geometric/data/dataset.py:158: UserWarning: The `pre_filter` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-fitering technique, make sure to delete '{self.processed_dir}' first
  warnings.warn(
Auto select gpus: [0]
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/main/home/mambaforge/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory ./lightning_logs/version_None/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-8ac7e914]
Traceback (most recent call last):
  File "/main/home/mambaforge/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py", line 292, in __getattr__
    return self[key]
KeyError: 'lr_factor'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/jgross/kinodata-docked-rescore/train_regressor.py", line 107, in <module>
    train_regressor(wandb.config)
  File "/home/jgross/kinodata-docked-rescore/train_regressor.py", line 99, in train_regressor
    trainer.fit(model, datamodule=data_module)
  File "/main/home/mambaforge/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 696, in fit
    self._call_and_handle_interrupt(
  File "/main/home/mambaforge/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/main/home/mambaforge/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 735, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/main/home/mambaforge/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1147, in _run
    self.strategy.setup(self)
  File "/main/home/mambaforge/lib/python3.9/site-packages/pytorch_lightning/strategies/single_device.py", line 74, in setup
    super().setup(trainer)
  File "/main/home/mambaforge/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py", line 153, in setup
    self.setup_optimizers(trainer)
  File "/main/home/mambaforge/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py", line 141, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs, self.optimizer_frequencies = _init_optimizers_and_lr_schedulers(
  File "/main/home/mambaforge/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py", line 179, in _init_optimizers_and_lr_schedulers
    optim_conf = model.trainer._call_lightning_module_hook("configure_optimizers", pl_module=model)
  File "/main/home/mambaforge/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1550, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/jgross/kinodata-docked-rescore/kinodata/model/model.py", line 52, in configure_optimizers
    factor=self.hparams.lr_factor,
  File "/main/home/mambaforge/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py", line 294, in __getattr__
    raise AttributeError(f'Missing attribute "{key}"') from exp
AttributeError: Missing attribute "lr_factor"
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: Synced scarlet-sun-42: https://wandb.ai/nextaids/kinodata-docked-rescore/runs/8ge9d36q
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230219_145817-8ge9d36q/logs
